{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgDm7dIcr8Ib",
        "outputId": "653ad0a1-779e-4016-b254-19f2651b80bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.32.3)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "     -------------------------------------- 147.9/147.9 KB 4.4 MB/s eta 0:00:00\n",
            "Collecting openai\n",
            "  Downloading openai-1.54.2-py3-none-any.whl (389 kB)\n",
            "     ------------------------------------- 389.3/389.3 KB 25.3 MB/s eta 0:00:00\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
            "     ---------------------------------------- 50.4/50.4 KB ? eta 0:00:00\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests) (2024.6.2)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Collecting anyio<5,>=3.5.0\n",
            "  Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
            "     ---------------------------------------- 90.4/90.4 KB ? eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\mathe\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.11.0)\n",
            "Collecting httpx<1,>=0.23.0\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "     ---------------------------------------- 76.4/76.4 KB 4.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.66.4)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting distro<2,>=1.7.0\n",
            "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Collecting jiter<1,>=0.4.0\n",
            "  Downloading jiter-0.7.0-cp310-none-win_amd64.whl (202 kB)\n",
            "     ------------------------------------- 202.0/202.0 KB 12.0 MB/s eta 0:00:00\n",
            "Collecting pydantic<3,>=1.9.0\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "     ------------------------------------- 434.9/434.9 KB 13.7 MB/s eta 0:00:00\n",
            "Collecting langchain-core<0.4.0,>=0.3.15\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "     ------------------------------------- 408.7/408.7 KB 24.9 MB/s eta 0:00:00\n",
            "Collecting tiktoken<1,>=0.7\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
            "     ------------------------------------- 884.2/884.2 KB 28.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mathe\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Collecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "     ---------------------------------------- 78.0/78.0 KB 4.5 MB/s eta 0:00:00\n",
            "Collecting h11<0.15,>=0.13\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "     ---------------------------------------- 58.3/58.3 KB ? eta 0:00:00\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mathe\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\mathe\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (24.0)\n",
            "Collecting langsmith<0.2.0,>=0.1.125\n",
            "  Downloading langsmith-0.1.140-py3-none-any.whl (304 kB)\n",
            "     ------------------------------------- 304.8/304.8 KB 18.4 MB/s eta 0:00:00\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
            "  Downloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.23.4\n",
            "  Downloading pydantic_core-2.23.4-cp310-none-win_amd64.whl (1.9 MB)\n",
            "     ---------------------------------------- 1.9/1.9 MB 30.3 MB/s eta 0:00:00\n",
            "Collecting annotated-types>=0.6.0\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting regex>=2022.1.18\n",
            "  Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl (274 kB)\n",
            "     ------------------------------------- 274.0/274.0 KB 17.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: colorama in c:\\users\\mathe\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14\n",
            "  Downloading orjson-3.10.11-cp310-none-win_amd64.whl (136 kB)\n",
            "     -------------------------------------- 136.4/136.4 KB 7.9 MB/s eta 0:00:00\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "     ---------------------------------------- 54.5/54.5 KB ? eta 0:00:00\n",
            "Installing collected packages: tenacity, soupsieve, sniffio, regex, pydantic-core, orjson, jsonpointer, jiter, h11, distro, annotated-types, tiktoken, requests-toolbelt, pydantic, jsonpatch, httpcore, beautifulsoup4, anyio, httpx, openai, langsmith, langchain-core, langchain-openai\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 beautifulsoup4-4.12.3 distro-1.9.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.7.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.15 langchain-openai-0.2.6 langsmith-0.1.140 openai-1.54.2 orjson-3.10.11 pydantic-2.9.2 pydantic-core-2.23.4 regex-2024.9.11 requests-toolbelt-1.0.0 sniffio-1.3.1 soupsieve-2.6 tenacity-9.0.0 tiktoken-0.8.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 openai langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHl6TYdzr_9E"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pGBb36usGz7"
      },
      "outputs": [],
      "source": [
        "url = \"https://dev.to/neuml/build-knowledge-graphs-with-llm-driven-entity-extraction-4hlm\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Uxv1dAhsHID"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_url(url=url):\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch the URL. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for script_or_style in soup([\"script\", \"style\"]):\n",
        "        script_or_style.decompose()\n",
        "    text = soup.get_text(separator = ' ')\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "kT_JpXtVtHUU",
        "outputId": "0110e5c4-922d-4722-8448-d3d31b9997eb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Build knowledge graphs with LLM-driven entity extraction - DEV Community\\nSkip to content\\nNavigation menu\\nSearch\\nPowered by\\nSearch\\nAlgolia\\nSearch\\nLog in\\nCreate account\\nDEV Community\\nClose\\nAdd reaction\\nLike\\nUnicorn\\nExploding Head\\nRaised Hands\\nFire\\nJump to Comments\\nSave\\nMore...\\nCopy link\\nCopy link\\nCopied to Clipboard\\nShare to X\\nShare to LinkedIn\\nShare to Facebook\\nShare to Mastodon\\nReport Abuse\\nDavid Mezzetti\\nfor\\nNeuML\\nPosted on\\nFeb 21\\nâ€¢ Updated on\\nApr 25\\nâ€¢ Originally published at\\nneuml.hashnode.dev\\nBuild knowledge graphs with LLM-driven entity extraction\\n# ai\\n# llm\\n# rag\\n# vectordatabase\\nTutorial series on txtai (66 Part Series)\\n1\\nTutorial series on txtai\\n2\\nBuild an Embeddings index with Hugging Face Datasets\\n...\\n62 more parts...\\n3\\nBuild an Embeddings index from a data source\\n4\\nAdd semantic search to Elasticsearch\\n5\\nExtractive QA with txtai\\n6\\nExtractive QA with Elasticsearch\\n7\\nApply labels with zero-shot classification\\n8\\ntxtai API Gallery\\n9\\nBuilding abstractive text summaries\\n10\\nExtract text from documents\\n11\\nTranscribe audio to text\\n12\\nTranslate text between languages\\n13\\nSimilarity search with images\\n14\\nRun pipeline workflows\\n15\\nDistributed embeddings cluster\\n16\\nTrain a text labeler\\n17\\nTrain without labels\\n18\\nExport and run models with ONNX\\n19\\nTrain a QA model\\n20\\nExtractive QA to build structured data\\n21\\nExport and run other machine learning models\\n22\\nTransform tabular data with composable workflows\\n23\\nTensor workflows\\n24\\nðŸ’¡ What\\'s new in txtai 4.0\\n25\\nGenerate image captions and detect objects\\n26\\nEntity extraction workflows\\n27\\nWorkflow Scheduling\\n28\\nPush notifications with workflows\\n29\\nAnatomy of a txtai index\\n30\\nEmbeddings SQL custom functions\\n31\\nNear duplicate image detection\\n32\\nModel explainability\\n33\\nQuery translation\\n34\\nBuild a QA database\\n35\\nPictures are a worth a thousand words\\n36\\nRun txtai in native code\\n37\\nEmbeddings index components\\n38\\nIntroducing the Semantic Graph\\n39\\nClassic Topic Modeling with BM25\\n40\\nText to speech generation\\n41\\nTrain a language model from scratch\\n42\\nPrompt-driven search with LLMs\\n43\\nEmbeddings in the Cloud\\n44\\nPrompt templates and task chains\\n45\\nCustomize your own embeddings database\\n46\\nðŸ’¡ What\\'s new in txtai 6.0\\n47\\nBuilding an efficient sparse keyword index in Python\\n48\\nBenefits of hybrid search\\n49\\nExternal database integration\\n50\\nAll about vector quantization\\n51\\nCustom API Endpoints\\n52\\nBuild RAG pipelines with txtai\\n53\\nIntegrate LLM Frameworks\\n54\\nAPI Authorization and Authentication\\n55\\nGenerate knowledge with Semantic Graphs and RAG\\n56\\nExternal vectorization\\n57\\nBuild knowledge graphs with LLM-driven entity extraction\\n58\\nAdvanced RAG with graph path traversal\\n59\\nWhat\\'s new in txtai 7.0\\n60\\nAdvanced RAG with guided generation\\n61\\nIntegrate txtai with Postgres\\n62\\nRAG with llama.cpp and external API services\\n63\\nHow RAG with txtai works\\n64\\nEmbeddings index format for open data access\\n65\\nSpeech to Speech RAG\\n66\\nGenerative Audio\\ntxtai\\nis an all-in-one embeddings database for semantic search, LLM orchestration and language model workflows.\\nEmbeddings databases are a union of vector indexes (sparse and dense), graph networks and relational databases. This enables vector search with SQL, topic modeling, retrieval augmented generation and more.\\nUp until txtai 7.0, semantic graphs only supported automatic relationship detection. Now relationships can be loaded directly into a txtai database. This article will demonstrate how to work with this new feature.\\nInstall dependencies\\nInstall\\ntxtai\\nand all dependencies.\\n# Install txtai\\npip install txtai[graph] autoawq\\nEnter fullscreen mode\\nExit fullscreen mode\\nLoad Wikipedia database\\nThe\\ntxtai-wikipedia\\ndatabase stores all Wikipedia article abstracts as of January 2024. This database is a great way to explore a wide variety of topics. It also has the number of page views integrated in, which enables pulling frequently viewed or popular articles on a topic.\\nFor this example, we\\'ll work with a couple articles related to\\nViking raids in France .\\nfrom\\ntxtai\\nimport\\nEmbeddings\\n# Load dataset\\nwikipedia\\n=\\nEmbeddings ()\\nwikipedia . load ( provider = \" huggingface-hub \" ,\\ncontainer = \" neuml/txtai-wikipedia \" )\\nquery\\n=\\n\"\"\"\\nSELECT id, text FROM txtai WHERE similar( \\' Viking raids in France \\' ) and percentile >= 0.5\\n\"\"\"\\nresults\\n=\\nwikipedia . search ( query ,\\n5 )\\nfor\\nx\\nin\\nresults :\\nprint ( x )\\nEnter fullscreen mode\\nExit fullscreen mode\\nLLM-driven entity extraction\\nNow that we have a couple relevant articles, let\\'s go through and run an entity extraction process. For this task, we\\'ll have a LLM prompt to do the work.\\nimport\\njson\\nfrom\\ntxtai\\nimport\\nLLM\\n# Load LLM\\nllm\\n=\\nLLM ( \" TheBloke/Mistral-7B-OpenOrca-AWQ \" )\\ndata\\n=\\n[]\\nfor\\nresult\\nin\\nresults :\\nprompt\\n=\\nf \"\"\" <|im_start|>system\\nYou are a friendly assistant. You answer questions from users.<|im_end|>\\n<|im_start|>user\\nExtract an entity relationship graph from the following text. Output as JSON\\nNodes must have label and type attributes. Edges must have source, target and relationship attributes.\\ntext:\\n{ result [ \\' text \\' ] }\\n<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\\ntry :\\ndata . append ( json . loads ( llm ( prompt ,\\nmaxlength = 4096 )))\\nexcept :\\npass\\nEnter fullscreen mode\\nExit fullscreen mode\\nBuild an embeddings database\\nNow that we\\'ve extracted entities from the documents, next we\\'ll review and load a graph network using these entity-relationships.\\ndef\\nstream ():\\nnodes\\n=\\n{}\\nfor\\nrow\\nin\\ndata . copy ():\\n# Create nodes\\nfor\\nnode\\nin\\nrow [ \" nodes \" ]:\\nif\\nnode [ \" label \" ]\\nnot\\nin\\nnodes :\\nnode [ \" id \" ]\\n=\\nlen ( nodes )\\nnodes [ node [ \" label \" ]]\\n=\\nnode\\nfor\\nedge\\nin\\nrow [ \" edges \" ]:\\nsource\\n=\\nnodes . get ( edge [ \" source \" ])\\ntarget\\n=\\nnodes . get ( edge [ \" target \" ])\\nif\\nsource\\nand\\ntarget :\\nif\\n\" relationships \"\\nnot\\nin\\nsource :\\nsource [ \" relationships \" ]\\n=\\n[]\\nsource [ \" relationships \" ]. append ({ \" id \" :\\ntarget [ \" id \" ],\\n\" relationship \" :\\nedge [ \" relationship \" ]})\\nreturn\\nnodes . values ()\\n# Create embeddings instance with a semantic graph\\nembeddings\\n=\\nEmbeddings (\\nautoid\\n=\\n\" uuid5 \" ,\\npath\\n=\\n\" intfloat/e5-base \" ,\\ninstructions\\n=\\n{\\n\" query \" :\\n\" query:\\n\" ,\\n\" data \" :\\n\" passage:\\n\"\\n},\\ncolumns\\n=\\n{\\n\" text \" :\\n\" label \"\\n},\\ncontent\\n=\\nTrue ,\\ngraph\\n=\\n{\\n\" approximate \" :\\nFalse ,\\n\" topics \" :\\n{}\\n}\\n)\\nembeddings . index ( stream ())\\nEnter fullscreen mode\\nExit fullscreen mode\\nShow the network\\nNow let\\'s visualize the entity-relationship network. This period might not be that familar to most, unless you\\'ve watched the Vikings TV series, in which case it should make sense.\\nimport\\nmatplotlib.pyplot\\nas\\nplt\\nimport\\nnetworkx\\nas\\nnx\\ndef\\nplot ( graph ):\\nlabels\\n=\\n{ x :\\nf \" { graph . attribute ( x ,\\n\\' text \\' ) }\\n( { x } ) \"\\nfor\\nx\\nin\\ngraph . scan ()}\\nlookup\\n=\\n{\\n\" Person \" :\\n\" #d32f2f \" ,\\n\" Location \" :\\n\" #0277bd \" ,\\n\" Event \" :\\n\" #e64980 \" ,\\n\" Role \" :\\n\" #757575 \"\\n}\\ncolors\\n=\\n[]\\nfor\\nx\\nin\\ngraph . scan ():\\nvalue\\n=\\nembeddings . search ( \" select type from txtai where id = :x \" ,\\nparameters = { \" x \" :\\nx })[ 0 ][ \" type \" ]\\ncolors . append ( lookup . get ( value ,\\n\" #7e57c2 \" ))\\noptions\\n=\\n{\\n\" node_size \" :\\n2000 ,\\n\" node_color \" :\\ncolors ,\\n\" edge_color \" :\\n\" #454545 \" ,\\n\" font_color \" :\\n\" #efefef \" ,\\n\" font_size \" :\\n11 ,\\n\" alpha \" :\\n1.0\\n}\\nfig ,\\nax\\n=\\nplt . subplots ( figsize = ( 20 ,\\n9 ))\\npos\\n=\\nnx . spring_layout ( graph . backend ,\\nseed = 0 ,\\nk = 3 ,\\niterations = 250 )\\nnx . draw_networkx ( graph . backend ,\\npos = pos ,\\nlabels = labels ,\\n** options )\\nax . set_facecolor ( \" #303030 \" )\\nax . axis ( \" off \" )\\nfig . set_facecolor ( \" #303030 \" )\\nplot ( embeddings . graph )\\nEnter fullscreen mode\\nExit fullscreen mode\\nGraph traversal\\nThe last thing we\\'ll cover is extracting a specific path from the graph. Let\\'s show a path from node 8 to node 5 requiring a specific relationship type to start.\\nA new feature of txtai 7.0 is the ability to return a graph of search results. This is a powerful addition as not only do we get the search results but we get how the search results relate to each other.\\n# Traverse graph looking for certain nodes and edge values\\ng\\n=\\nembeddings . graph . search ( \"\"\"\\nMATCH P=(A{id: 8})-[R1]->()-[*1..3]->(D{id:5})\\nWHERE\\nR1.relationship ==\\n\" has_object \"\\nRETURN P\\n\"\"\" ,\\ngraph = True )\\nplot ( g )\\nEnter fullscreen mode\\nExit fullscreen mode\\nWrapping up\\nThis article showed how knowledge graphs can be created with LLM-driven entity extraction. Automatically derived relationships via semantic similarity and manually specified ones are a powerful combination!\\nTutorial series on txtai (66 Part Series)\\n1\\nTutorial series on txtai\\n2\\nBuild an Embeddings index with Hugging Face Datasets\\n...\\n62 more parts...\\n3\\nBuild an Embeddings index from a data source\\n4\\nAdd semantic search to Elasticsearch\\n5\\nExtractive QA with txtai\\n6\\nExtractive QA with Elasticsearch\\n7\\nApply labels with zero-shot classification\\n8\\ntxtai API Gallery\\n9\\nBuilding abstractive text summaries\\n10\\nExtract text from documents\\n11\\nTranscribe audio to text\\n12\\nTranslate text between languages\\n13\\nSimilarity search with images\\n14\\nRun pipeline workflows\\n15\\nDistributed embeddings cluster\\n16\\nTrain a text labeler\\n17\\nTrain without labels\\n18\\nExport and run models with ONNX\\n19\\nTrain a QA model\\n20\\nExtractive QA to build structured data\\n21\\nExport and run other machine learning models\\n22\\nTransform tabular data with composable workflows\\n23\\nTensor workflows\\n24\\nðŸ’¡ What\\'s new in txtai 4.0\\n25\\nGenerate image captions and detect objects\\n26\\nEntity extraction workflows\\n27\\nWorkflow Scheduling\\n28\\nPush notifications with workflows\\n29\\nAnatomy of a txtai index\\n30\\nEmbeddings SQL custom functions\\n31\\nNear duplicate image detection\\n32\\nModel explainability\\n33\\nQuery translation\\n34\\nBuild a QA database\\n35\\nPictures are a worth a thousand words\\n36\\nRun txtai in native code\\n37\\nEmbeddings index components\\n38\\nIntroducing the Semantic Graph\\n39\\nClassic Topic Modeling with BM25\\n40\\nText to speech generation\\n41\\nTrain a language model from scratch\\n42\\nPrompt-driven search with LLMs\\n43\\nEmbeddings in the Cloud\\n44\\nPrompt templates and task chains\\n45\\nCustomize your own embeddings database\\n46\\nðŸ’¡ What\\'s new in txtai 6.0\\n47\\nBuilding an efficient sparse keyword index in Python\\n48\\nBenefits of hybrid search\\n49\\nExternal database integration\\n50\\nAll about vector quantization\\n51\\nCustom API Endpoints\\n52\\nBuild RAG pipelines with txtai\\n53\\nIntegrate LLM Frameworks\\n54\\nAPI Authorization and Authentication\\n55\\nGenerate knowledge with Semantic Graphs and RAG\\n56\\nExternal vectorization\\n57\\nBuild knowledge graphs with LLM-driven entity extraction\\n58\\nAdvanced RAG with graph path traversal\\n59\\nWhat\\'s new in txtai 7.0\\n60\\nAdvanced RAG with guided generation\\n61\\nIntegrate txtai with Postgres\\n62\\nRAG with llama.cpp and external API services\\n63\\nHow RAG with txtai works\\n64\\nEmbeddings index format for open data access\\n65\\nSpeech to Speech RAG\\n66\\nGenerative Audio\\nTop comments\\n(0)\\nSubscribe\\nPersonal\\nTrusted User\\nCreate template\\nTemplates let you quickly answer FAQs or store snippets for re-use.\\nSubmit\\nPreview\\nDismiss\\nCode of Conduct\\nâ€¢\\nReport abuse\\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\'s\\npermalink .\\nHide child comments as well\\nConfirm\\nFor further actions, you may consider blocking this person and/or\\nreporting abuse\\nRead next\\nUnearthing Universal Feature Geometries: Sparse Autoencoders Reveal Crystal-like and Modular Structures\\nMike Young -\\nOct 31\\nLLMs Know More Than They Show: Intrinsic Representation of Hallucinations Revealed\\nMike Young -\\nOct 31\\nðŸš€ Getting Started with AI-Powered Coding Assistants ðŸš€\\nAshish prajapati -\\nOct 31\\nUnderstanding OpenAIâ€™s ChatGPT History Policy: What It Means for Users\\naditi gupta1633 -\\nOct 31\\nNeuML\\nFollow\\nMore from\\nNeuML\\nGenerative Audio\\n# ai\\n# llm\\n# rag\\n# vectordatabase\\nSpeech to Speech RAG\\n# ai\\n# llm\\n# rag\\n# vectordatabase\\nEmbeddings index format for open data access\\n# ai\\n# llm\\n# rag\\n# vectordatabase\\nThank you to our Diamond Sponsor\\nNeon\\nfor supporting our community.\\nDEV Community\\nâ€” A constructive and inclusive social network for software developers. With you every step of your journey.\\nHome\\nDEV++\\nPodcasts\\nVideos\\nTags\\nDEV Help\\nForem Shop\\nAdvertise on DEV\\nDEV Challenges\\nDEV Showcase\\nAbout\\nContact\\nFree Postgres Database\\nGuides\\nSoftware comparisons\\nCode of Conduct\\nPrivacy Policy\\nTerms of use\\nBuilt on\\nForem\\nâ€” the\\nopen source\\nsoftware that powers\\nDEV\\nand other inclusive communities.\\nMade with love and\\nRuby on Rails . DEV Community\\nÂ©\\n2016 - 2024.\\nWe\\'re a place where coders share, stay up-to-date and grow their careers.\\nLog in\\nCreate account'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_text_from_url()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eIEqgAlxBrL"
      },
      "source": [
        "Azure gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_WhiPciz9VT",
        "outputId": "fc0d948b-e289-4841-bab7-0bb08859a865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.6)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGE_jTcmz8bN"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SDDFVr60VQV"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain>=0.0.264"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tTZ6yyvyNDM"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import AzureChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJutiGHmw6Bb"
      },
      "outputs": [],
      "source": [
        "client = AzureChatOpenAI(\n",
        "    azure_endpoint = \"url\",\n",
        "    api_key = \"secret\",\n",
        "    api_version = \"2024-02-15-preview\",\n",
        "    deployment_name = \"gpt-4o-mini\",\n",
        "    max_retries=0\n",
        ")\n",
        "\n",
        "def translate_article(text, lang):\n",
        "  messages = [\n",
        "       (\"system\", \"VocÃª atua como tradutor de textos\"),\n",
        "       (\"user\", f\"Traduza o {text} para o idioma {lang} e resonda em markdown\")\n",
        "  ]\n",
        "\n",
        "  response = client.invoke(messages)\n",
        "  print(response.content)\n",
        "  return response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3UWlTfdy17b",
        "outputId": "3c2fc185-dffb-4741-adbf-2b3cbced733d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estou tentando usar o copiloto de IA para traduzir meu texto.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Estou tentando usar o copiloto de IA para traduzir meu texto.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 43, 'total_tokens': 57, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_d54531d9eb', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-ec93d834-03bb-44e0-91c7-78ae2161fc6d-0', usage_metadata={'input_tokens': 43, 'output_tokens': 14, 'total_tokens': 57, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translate_article(\"i'm trying to use the ai copilot to translate my text\", \"portugues\")\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "W4fTpeRN09Gl",
        "outputId": "44e531f8-48aa-45ea-d836-262d7d5f1c97"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-6e98f13176b4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://dev.to/neuml/build-knowledge-graphs-with-llm-driven-entity-extraction-4hlm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"portugues\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-aee2305e7b27>\u001b[0m in \u001b[0;36mtranslate_article\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m     13\u001b[0m   ]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    827\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         )\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-02-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"
          ]
        }
      ],
      "source": [
        "url = \"https://dev.to/neuml/build-knowledge-graphs-with-llm-driven-entity-extraction-4hlm\"\n",
        "text = extract_text_from_url(url)\n",
        "article = translate_article(text, \"portugues\")\n",
        "\n",
        "print(article)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
